{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hCRyZZqdhCI"
   },
   "source": [
    "# **Тренировочный ноутбук для Tacotron2(multispeaker) + HiFiTTS** \n",
    "Обучение модели [Tacotron2](https://github.com/NVIDIA/tacotron2) (модифицированной под многоголосовую речь), на датасете HiFiTTS ([Пайплайн](https://github.com/ViktorKrasnorutskiy/tacotron2_hifitts/blob/main/train/prepare_dataset.ipynb) по подготовке датасета [HiFiTTS](http://www.openslr.org/109/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7OYsBeTlGrN4"
   },
   "outputs": [],
   "source": [
    "# На бесплатной версии Colab желательно получить Tesla T4 (Tesla K80 - хуже)\n",
    "# Сброс полученной карты - \"Среда_выполнения/Отключиться_от_среды_выполнения_и_удалить_ее\"\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5paIADJGvIs"
   },
   "outputs": [],
   "source": [
    "# Подключение Гугл Диска\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1QPjIXhtGw1M"
   },
   "outputs": [],
   "source": [
    "# Скачивание репозитория Tacotron2\n",
    "%tensorflow_version 1.x\n",
    "import os\n",
    "\n",
    "!git clone -q https://github.com/NVIDIA/tacotron2\n",
    "os.chdir('tacotron2')\n",
    "!git submodule init\n",
    "!git submodule update\n",
    "!pip install -q unidecode tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTYy8MU_G4Uz"
   },
   "outputs": [],
   "source": [
    "# Распаковка заранее загруженного zip архива с мел-спектограммами датасета\n",
    "# Необходимо загрузить данный архив на Гугл Диск\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "zipped_dataset = '/content/drive/MyDrive/data/mels.zip'\n",
    "z = zipfile.ZipFile(zipped_dataset, 'r')\n",
    "z.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "TUMVNU-N3hpL"
   },
   "outputs": [],
   "source": [
    "#@title Модифицированные модули Tacotron2 под Multispeaker\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from numpy import finfo\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from model import LocationLayer, Attention, Prenet, Postnet, Encoder\n",
    "from torch.utils.data import DataLoader\n",
    "from text import text_to_sequence, symbols\n",
    "import layers\n",
    "import torch.utils.data\n",
    "import random\n",
    "from loss_function import Tacotron2Loss\n",
    "from logger import Tacotron2Logger\n",
    "from hparams import create_hparams\n",
    "from math import sqrt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from layers import ConvNorm, LinearNorm\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def to_gpu(x):\n",
    "    x = x.contiguous()\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda(non_blocking=True)\n",
    "    return torch.autograd.Variable(x)\n",
    "\n",
    "\n",
    "def create_hparams(hparams_string=None, verbose=False):\n",
    "    \"\"\"Create model hyperparameters. Parse nondefault from given string.\"\"\"\n",
    "    hparams = tf.contrib.training.HParams(\n",
    "        ################################\n",
    "        # Experiment Parameters        #\n",
    "        ################################\n",
    "        epochs=500,\n",
    "        iters_per_checkpoint=1000,\n",
    "        seed=1234,\n",
    "        dynamic_loss_scaling=True,\n",
    "        fp16_run=False,\n",
    "        distributed_run=False,\n",
    "        dist_backend=\"nccl\",\n",
    "        dist_url=\"tcp://localhost:54321\",\n",
    "        cudnn_enabled=True,\n",
    "        cudnn_benchmark=False,\n",
    "        ignore_layers=['embedding.weight'],\n",
    "\n",
    "        ################################\n",
    "        # Data Parameters             #\n",
    "        ################################\n",
    "        load_mel_from_disk=True,\n",
    "        training_files='filelists/train.txt',\n",
    "        validation_files='filelists/test.txt',\n",
    "        text_cleaners=['english_cleaners'],\n",
    "\n",
    "        ################################\n",
    "        # Audio Parameters             #\n",
    "        ################################\n",
    "        max_wav_value=32768.0,\n",
    "        sampling_rate= 22050,\n",
    "        filter_length=1024,\n",
    "        hop_length=256,\n",
    "        win_length=1024,\n",
    "        n_mel_channels=80,\n",
    "        mel_fmin=0.0,\n",
    "        mel_fmax=8000.0,\n",
    "\n",
    "        ################################\n",
    "        # Model Parameters             #\n",
    "        ################################\n",
    "        n_symbols=len(symbols),\n",
    "        symbols_embedding_dim=512,\n",
    "\n",
    "        # ADD SPEAKER\n",
    "        n_speakers=10,\n",
    "        speakers_embedding_dim=2,\n",
    "        # ADD SPEAKER\n",
    "\n",
    "        # Encoder parameters\n",
    "        encoder_kernel_size=5,\n",
    "        encoder_n_convolutions=3,\n",
    "        encoder_embedding_dim=512,\n",
    "\n",
    "        # Decoder parameters\n",
    "        n_frames_per_step=1,  # currently only 1 is supported\n",
    "        decoder_rnn_dim=1024,\n",
    "        prenet_dim=256,\n",
    "        max_decoder_steps=1000,\n",
    "        gate_threshold=0.5,\n",
    "        p_attention_dropout=0.1,\n",
    "        p_decoder_dropout=0.1,\n",
    "\n",
    "        # Attention parameters\n",
    "        attention_rnn_dim=1024,\n",
    "        attention_dim=128,\n",
    "\n",
    "        # Location Layer parameters\n",
    "        attention_location_n_filters=32,\n",
    "        attention_location_kernel_size=31,\n",
    "\n",
    "        # Mel-post processing network parameters\n",
    "        postnet_embedding_dim=512,\n",
    "        postnet_kernel_size=5,\n",
    "        postnet_n_convolutions=5,\n",
    "\n",
    "        ################################\n",
    "        # Optimization Hyperparameters #\n",
    "        ################################\n",
    "        use_saved_learning_rate=False,\n",
    "        learning_rate=(1e-3)/(2**0.5), #1e-3,\n",
    "        weight_decay=1e-6,\n",
    "        grad_clip_thresh=1.0,\n",
    "        batch_size=64,\n",
    "        mask_padding=True  # set model's padded outputs to padded values\n",
    "    )\n",
    "    if hparams_string:\n",
    "        tf.logging.info('Parsing command line hparams: %s', hparams_string)\n",
    "        hparams.parse(hparams_string)\n",
    "    if verbose:\n",
    "        tf.logging.info('Final parsed hparams: %s', hparams.values())\n",
    "    return hparams\n",
    "\n",
    "\n",
    "def get_mask_from_lengths(lengths):\n",
    "    max_len = torch.max(lengths).item()\n",
    "    ids = torch.arange(0, max_len, out=torch.LongTensor(max_len).to(device))\n",
    "    mask = (ids < lengths.unsqueeze(1)).bool()\n",
    "    return mask\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hparams, encoder_out_embedding_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_mel_channels = hparams.n_mel_channels\n",
    "        self.n_frames_per_step = hparams.n_frames_per_step\n",
    "        ''' ADD SPEAKERS EMB DIM TO ENCODER OUT DIM '''\n",
    "        self.speakers_embedding_dim = hparams.speakers_embedding_dim\n",
    "        self.encoder_embedding_dim = encoder_out_embedding_dim\n",
    "        ''' ADD SPEAKERS EMB DIM TO ENCODER OUT DIM '''\n",
    "        self.attention_rnn_dim = hparams.attention_rnn_dim\n",
    "        self.decoder_rnn_dim = hparams.decoder_rnn_dim\n",
    "        self.prenet_dim = hparams.prenet_dim\n",
    "        self.max_decoder_steps = hparams.max_decoder_steps\n",
    "        self.gate_threshold = hparams.gate_threshold\n",
    "        self.p_attention_dropout = hparams.p_attention_dropout\n",
    "        self.p_decoder_dropout = hparams.p_decoder_dropout\n",
    "        self.prenet = Prenet(\n",
    "            hparams.n_mel_channels * hparams.n_frames_per_step,\n",
    "            [hparams.prenet_dim, hparams.prenet_dim])\n",
    "        self.attention_rnn = nn.LSTMCell(\n",
    "            hparams.prenet_dim + encoder_out_embedding_dim, # EDITED\n",
    "            hparams.attention_rnn_dim)\n",
    "        self.attention_layer = Attention(\n",
    "            hparams.attention_rnn_dim, encoder_out_embedding_dim, # EDITED\n",
    "            hparams.attention_dim, hparams.attention_location_n_filters,\n",
    "            hparams.attention_location_kernel_size)\n",
    "        self.decoder_rnn = nn.LSTMCell(\n",
    "            hparams.attention_rnn_dim + encoder_out_embedding_dim, # EDITED\n",
    "            hparams.decoder_rnn_dim, 1)\n",
    "        self.linear_projection = LinearNorm(\n",
    "            hparams.decoder_rnn_dim + encoder_out_embedding_dim, # EDITED\n",
    "            hparams.n_mel_channels * hparams.n_frames_per_step)\n",
    "        self.gate_layer = LinearNorm(\n",
    "            hparams.decoder_rnn_dim + encoder_out_embedding_dim, # EDITED\n",
    "            1, bias=True, w_init_gain='sigmoid')\n",
    "\n",
    "    def get_go_frame(self, memory):\n",
    "        B = memory.size(0)\n",
    "        decoder_input = Variable(memory.data.new(\n",
    "            B, self.n_mel_channels * self.n_frames_per_step).zero_())\n",
    "        return decoder_input\n",
    "\n",
    "    def initialize_decoder_states(self, memory, mask):\n",
    "        B = memory.size(0)\n",
    "        MAX_TIME = memory.size(1)\n",
    "        self.attention_hidden = Variable(memory.data.new(\n",
    "            B, self.attention_rnn_dim).zero_())\n",
    "        self.attention_cell = Variable(memory.data.new(\n",
    "            B, self.attention_rnn_dim).zero_())\n",
    "        self.decoder_hidden = Variable(memory.data.new(\n",
    "            B, self.decoder_rnn_dim).zero_())\n",
    "        self.decoder_cell = Variable(memory.data.new(\n",
    "            B, self.decoder_rnn_dim).zero_())\n",
    "        self.attention_weights = Variable(memory.data.new(\n",
    "            B, MAX_TIME).zero_())\n",
    "        self.attention_weights_cum = Variable(memory.data.new(\n",
    "            B, MAX_TIME).zero_())\n",
    "        self.attention_context = Variable(memory.data.new(\n",
    "            B,\n",
    "            self.encoder_embedding_dim\n",
    "        ).zero_())\n",
    "        self.memory = memory\n",
    "        self.processed_memory = self.attention_layer.memory_layer(memory)\n",
    "        self.mask = mask\n",
    "\n",
    "    def parse_decoder_inputs(self, decoder_inputs):\n",
    "        decoder_inputs = decoder_inputs.transpose(1, 2)\n",
    "        decoder_inputs = decoder_inputs.view(\n",
    "            decoder_inputs.size(0),\n",
    "            int(decoder_inputs.size(1)/self.n_frames_per_step), -1)\n",
    "        decoder_inputs = decoder_inputs.transpose(0, 1)\n",
    "        return decoder_inputs\n",
    "\n",
    "    def parse_decoder_outputs(self, mel_outputs, gate_outputs, alignments):\n",
    "        alignments = torch.stack(alignments).transpose(0, 1)\n",
    "        gate_outputs = torch.stack(gate_outputs).transpose(0, 1)\n",
    "        gate_outputs = gate_outputs.contiguous()\n",
    "        mel_outputs = torch.stack(mel_outputs).transpose(0, 1).contiguous()\n",
    "        mel_outputs = mel_outputs.view(\n",
    "            mel_outputs.size(0), -1, self.n_mel_channels)\n",
    "        mel_outputs = mel_outputs.transpose(1, 2)\n",
    "        return mel_outputs, gate_outputs, alignments\n",
    "\n",
    "    def decode(self, decoder_input):\n",
    "        cell_input = torch.cat((decoder_input, self.attention_context), -1)\n",
    "        self.attention_hidden, self.attention_cell = self.attention_rnn(\n",
    "            cell_input, (self.attention_hidden, self.attention_cell))\n",
    "        self.attention_hidden = F.dropout(\n",
    "            self.attention_hidden, self.p_attention_dropout, self.training)\n",
    "        attention_weights_cat = torch.cat(\n",
    "            (self.attention_weights.unsqueeze(1),\n",
    "             self.attention_weights_cum.unsqueeze(1)), dim=1)\n",
    "        self.attention_context, self.attention_weights = self.attention_layer(\n",
    "            self.attention_hidden, self.memory, self.processed_memory,\n",
    "            attention_weights_cat, self.mask)\n",
    "        self.attention_weights_cum += self.attention_weights\n",
    "        decoder_input = torch.cat(\n",
    "            (self.attention_hidden, self.attention_context), -1)\n",
    "        self.decoder_hidden, self.decoder_cell = self.decoder_rnn(\n",
    "            decoder_input, (self.decoder_hidden, self.decoder_cell))\n",
    "        self.decoder_hidden = F.dropout(\n",
    "            self.decoder_hidden, self.p_decoder_dropout, self.training)\n",
    "        decoder_hidden_attention_context = torch.cat(\n",
    "            (self.decoder_hidden, self.attention_context), dim=1)\n",
    "        decoder_output = self.linear_projection(\n",
    "            decoder_hidden_attention_context)\n",
    "        gate_prediction = self.gate_layer(decoder_hidden_attention_context)\n",
    "        return decoder_output, gate_prediction, self.attention_weights\n",
    "\n",
    "    def forward(self, memory, decoder_inputs, memory_lengths):\n",
    "        decoder_input = self.get_go_frame(memory).unsqueeze(0)\n",
    "        decoder_inputs = self.parse_decoder_inputs(decoder_inputs)\n",
    "        decoder_inputs = torch.cat((decoder_input, decoder_inputs), dim=0)\n",
    "        decoder_inputs = self.prenet(decoder_inputs)\n",
    "        self.initialize_decoder_states(\n",
    "            memory, mask=~get_mask_from_lengths(memory_lengths))\n",
    "        mel_outputs, gate_outputs, alignments = [], [], []\n",
    "        while len(mel_outputs) < decoder_inputs.size(0) - 1:\n",
    "            decoder_input = decoder_inputs[len(mel_outputs)]\n",
    "            mel_output, gate_output, attention_weights = self.decode(\n",
    "                decoder_input)\n",
    "            mel_outputs += [mel_output.squeeze(1)]\n",
    "            gate_outputs += [gate_output.squeeze(1)]\n",
    "            alignments += [attention_weights]\n",
    "        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
    "            mel_outputs, gate_outputs, alignments)\n",
    "        return mel_outputs, gate_outputs, alignments\n",
    "\n",
    "    def inference(self, memory):\n",
    "        decoder_input = self.get_go_frame(memory)\n",
    "        self.initialize_decoder_states(memory, mask=None)\n",
    "        mel_outputs, gate_outputs, alignments = [], [], []\n",
    "        while True:\n",
    "            decoder_input = self.prenet(decoder_input)\n",
    "            mel_output, gate_output, alignment = self.decode(decoder_input)\n",
    "            mel_outputs += [mel_output.squeeze(1)]\n",
    "            mel_outputs += [mel_output.squeeze(1)]\n",
    "            gate_outputs += [gate_output]\n",
    "            alignments += [alignment]\n",
    "            if torch.sigmoid(gate_output.data) > self.gate_threshold:\n",
    "                break\n",
    "            elif len(mel_outputs) == self.max_decoder_steps:\n",
    "                print(\"Warning! Reached max decoder steps\")\n",
    "                break\n",
    "            decoder_input = mel_output\n",
    "        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
    "            mel_outputs, gate_outputs, alignments)\n",
    "        return mel_outputs, gate_outputs, alignments\n",
    "\n",
    "\n",
    "class Tacotron2(nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super(Tacotron2, self).__init__()\n",
    "        self.mask_padding = hparams.mask_padding\n",
    "        self.fp16_run = hparams.fp16_run\n",
    "        self.n_mel_channels = hparams.n_mel_channels\n",
    "        self.n_frames_per_step = hparams.n_frames_per_step\n",
    "        self.embedding = nn.Embedding(\n",
    "            hparams.n_symbols, hparams.symbols_embedding_dim)\n",
    "        std = sqrt(2.0 / (hparams.n_symbols + hparams.symbols_embedding_dim))\n",
    "        val = sqrt(3.0) * std  # uniform bounds for std\n",
    "        self.embedding.weight.data.uniform_(-val, val)\n",
    "        ''' ADD SPEAKER '''\n",
    "        self.speakers_embedding = nn.Embedding(\n",
    "            hparams.n_speakers,\n",
    "            hparams.speakers_embedding_dim)\n",
    "        encoder_out_embedding_dim = hparams.encoder_embedding_dim +  hparams.speakers_embedding_dim\n",
    "        ''' ADD SPEAKER '''\n",
    "        self.encoder = Encoder(hparams)\n",
    "        self.decoder = Decoder(hparams, encoder_out_embedding_dim)\n",
    "        self.postnet = Postnet(hparams)\n",
    "\n",
    "    def parse_batch(self, batch):\n",
    "        text_padded, input_lengths, mel_padded, gate_padded, \\\n",
    "            output_lengths, speaker_ids = batch\n",
    "        text_padded = to_gpu(text_padded).long()\n",
    "        input_lengths = to_gpu(input_lengths).long()\n",
    "        max_len = torch.max(input_lengths.data).item()\n",
    "        mel_padded = to_gpu(mel_padded).float()\n",
    "        gate_padded = to_gpu(gate_padded).float()\n",
    "        output_lengths = to_gpu(output_lengths).long()\n",
    "        ''' ADD SPEAKER '''\n",
    "        speaker_ids = to_gpu(speaker_ids).long()\n",
    "        ''' ADD SPEAKER '''\n",
    "        return (\n",
    "            (text_padded, input_lengths, mel_padded, max_len, output_lengths, speaker_ids),\n",
    "            (mel_padded, gate_padded))\n",
    "\n",
    "    def parse_output(self, outputs, output_lengths=None):\n",
    "        if self.mask_padding and output_lengths is not None:\n",
    "            mask = ~get_mask_from_lengths(output_lengths)\n",
    "            mask = mask.expand(self.n_mel_channels, mask.size(0), mask.size(1))\n",
    "            mask = mask.permute(1, 0, 2)\n",
    "            outputs[0].data.masked_fill_(mask, 0.0)\n",
    "            outputs[1].data.masked_fill_(mask, 0.0)\n",
    "            outputs[2].data.masked_fill_(mask[:, 0, :], 1e3)  # gate energies\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        text_inputs, text_lengths, mels, max_len, output_lengths, speaker_ids = inputs\n",
    "        text_lengths, output_lengths = text_lengths.data, output_lengths.data\n",
    "        outputs = []\n",
    "        embedded_inputs = self.embedding(text_inputs).transpose(1, 2)\n",
    "        encoder_outputs = self.encoder(embedded_inputs, text_lengths)\n",
    "        outputs.append(encoder_outputs)\n",
    "        ''' ADD SPEAKER '''\n",
    "        speaker_ids = speaker_ids.unsqueeze(1)\n",
    "        embedded_speakers = self.speakers_embedding(speaker_ids)\n",
    "        embedded_speakers = embedded_speakers.expand(-1, max_len, -1)\n",
    "        outputs.append(embedded_speakers)\n",
    "        ''' ADD SPEAKER '''\n",
    "        merged_outputs = torch.cat(outputs, -1)\n",
    "        mel_outputs, gate_outputs, alignments = self.decoder(\n",
    "            merged_outputs, mels, memory_lengths=text_lengths)\n",
    "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
    "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "        return self.parse_output(\n",
    "            [mel_outputs, mel_outputs_postnet, gate_outputs, alignments],\n",
    "            output_lengths)\n",
    "\n",
    "    def inference(self, inputs, speaker_id):\n",
    "        outputs = []\n",
    "        embedded_inputs = self.embedding(inputs).transpose(1, 2)\n",
    "        encoder_outputs = self.encoder.inference(embedded_inputs)\n",
    "        outputs.append(encoder_outputs)\n",
    "        ''' ADD SPEAKER '''\n",
    "        speaker_id = torch.IntTensor([speaker_id])\n",
    "        speaker_id = speaker_id.unsqueeze(1)\n",
    "        embedded_speaker = self.speakers_embedding(speaker_id)\n",
    "        embedded_speaker = embedded_speaker.expand(-1, encoder_outputs.shape[1], -1)\n",
    "        outputs.append(embedded_speaker)\n",
    "        ''' ADD SPEAKER '''\n",
    "        merged_outputs = torch.cat(outputs, -1)\n",
    "        mel_outputs, gate_outputs, alignments = self.decoder.inference(\n",
    "            merged_outputs)\n",
    "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
    "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "        outputs = self.parse_output(\n",
    "            [mel_outputs, mel_outputs_postnet, gate_outputs, alignments])\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def load_filepaths_and_text(filename, split=\"|\"):\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        filepaths_and_text = [line.strip().split(split) for line in f]\n",
    "    return filepaths_and_text\n",
    "\n",
    "\n",
    "class TextMelLoader(torch.utils.data.Dataset):\n",
    "    def __init__(self, audiopaths_and_text, hparams):\n",
    "        self.audiopaths_and_text = load_filepaths_and_text(audiopaths_and_text)\n",
    "        self.text_cleaners = hparams.text_cleaners\n",
    "        self.max_wav_value = hparams.max_wav_value\n",
    "        self.sampling_rate = hparams.sampling_rate\n",
    "        self.load_mel_from_disk = hparams.load_mel_from_disk\n",
    "        self.stft = layers.TacotronSTFT(\n",
    "            hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
    "            hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
    "            hparams.mel_fmax)\n",
    "        random.seed(hparams.seed)\n",
    "        random.shuffle(self.audiopaths_and_text)\n",
    "\n",
    "    def get_mel_text_pair(self, audiopath_and_text):\n",
    "        audiopath, text = audiopath_and_text[0], audiopath_and_text[1]\n",
    "        ''' ADD SPEAKER '''\n",
    "        speaker_id = audiopath_and_text[2]\n",
    "        ''' ADD SPEAKER '''\n",
    "        text = self.get_text(text)\n",
    "        mel = self.get_mel(audiopath)\n",
    "        return (text, mel, speaker_id)\n",
    "\n",
    "    def get_mel(self, filename):\n",
    "        mel_ndarray = np.load(filename + '.npy')\n",
    "        melspec = torch.from_numpy(mel_ndarray)\n",
    "        assert melspec.size(0) == self.stft.n_mel_channels, (\n",
    "            'Mel dimension mismatch: given {}, expected {}'.format(\n",
    "            melspec.size(0), self.stft.n_mel_channels))\n",
    "        return melspec\n",
    "\n",
    "    def get_text(self, text):\n",
    "        text_norm = torch.IntTensor(text_to_sequence(text, self.text_cleaners))\n",
    "        return text_norm\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.get_mel_text_pair(self.audiopaths_and_text[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audiopaths_and_text)\n",
    "\n",
    "\n",
    "class TextMelCollate():\n",
    "    def __init__(self, n_frames_per_step):\n",
    "        self.n_frames_per_step = n_frames_per_step\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        input_lengths, ids_sorted_decreasing = torch.sort(\n",
    "            torch.LongTensor([len(x[0]) for x in batch]),\n",
    "            dim=0, descending=True)\n",
    "        max_input_len = input_lengths[0]\n",
    "        text_padded = torch.LongTensor(len(batch), max_input_len)\n",
    "        text_padded.zero_()\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            text = batch[ids_sorted_decreasing[i]][0]\n",
    "            text_padded[i, :text.size(0)] = text\n",
    "        num_mels = batch[0][1].size(0)\n",
    "        max_target_len = max([x[1].size(1) for x in batch])\n",
    "        if max_target_len % self.n_frames_per_step != 0:\n",
    "            max_target_len += self.n_frames_per_step - max_target_len % self.n_frames_per_step\n",
    "            assert max_target_len % self.n_frames_per_step == 0\n",
    "        mel_padded = torch.FloatTensor(len(batch), num_mels, max_target_len)\n",
    "        mel_padded.zero_()\n",
    "        gate_padded = torch.FloatTensor(len(batch), max_target_len)\n",
    "        gate_padded.zero_()\n",
    "        output_lengths = torch.LongTensor(len(batch))\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            mel = batch[ids_sorted_decreasing[i]][1]\n",
    "            mel_padded[i, :, :mel.size(1)] = mel\n",
    "            gate_padded[i, mel.size(1)-1:] = 1\n",
    "            output_lengths[i] = mel.size(1)\n",
    "        ''' ADD SPEAKER '''\n",
    "        speaker_ids = []\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            speaker_id = batch[ids_sorted_decreasing[i]][2]\n",
    "            speaker_ids.append(int(speaker_id))\n",
    "        speaker_ids = torch.Tensor(speaker_ids)\n",
    "        ''' ADD SPEAKER '''\n",
    "        return text_padded, input_lengths, mel_padded, gate_padded, \\\n",
    "            output_lengths, speaker_ids\n",
    "\n",
    "\n",
    "def prepare_dataloaders(hparams):\n",
    "    trainset = TextMelLoader(hparams.training_files, hparams)\n",
    "    valset = TextMelLoader(hparams.validation_files, hparams)\n",
    "    collate_fn = TextMelCollate(hparams.n_frames_per_step)\n",
    "    train_loader = DataLoader(trainset, num_workers=1, shuffle=True,\n",
    "                              sampler=None,\n",
    "                              batch_size=hparams.batch_size, pin_memory=False,\n",
    "                              drop_last=True, collate_fn=collate_fn)\n",
    "    return train_loader, valset, collate_fn\n",
    "\n",
    "\n",
    "def prepare_directories_and_logger(output_directory, log_directory, rank):\n",
    "    if rank == 0:\n",
    "        if not os.path.isdir(output_directory):\n",
    "            os.makedirs(output_directory)\n",
    "            os.chmod(output_directory, 0o775)\n",
    "        logger = Tacotron2Logger(os.path.join(output_directory, log_directory))\n",
    "    else:\n",
    "        logger = None\n",
    "    return logger\n",
    "\n",
    "\n",
    "def load_model(hparams):\n",
    "    model = Tacotron2(hparams).to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "def warm_start_model(checkpoint_path, model, ignore_layers):\n",
    "    assert os.path.isfile(checkpoint_path)\n",
    "    print(\"Warm starting model from checkpoint '{}'\".format(checkpoint_path))\n",
    "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = checkpoint_dict['state_dict']\n",
    "    if len(ignore_layers) > 0:\n",
    "        model_dict = {k: v for k, v in model_dict.items()\n",
    "                      if k not in ignore_layers}\n",
    "        dummy_dict = model.state_dict()\n",
    "        dummy_dict.update(model_dict)\n",
    "        model_dict = dummy_dict\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    assert os.path.isfile(checkpoint_path)\n",
    "    print(\"Loading checkpoint '{}'\".format(checkpoint_path))\n",
    "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint_dict['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
    "    learning_rate = checkpoint_dict['learning_rate']\n",
    "    iteration = checkpoint_dict['iteration']\n",
    "    print(\"Loaded checkpoint '{}' from iteration {}\" .format(\n",
    "        checkpoint_path, iteration))\n",
    "    return model, optimizer, learning_rate, iteration\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, learning_rate, iteration, filepath, drive_path,\n",
    "                    iters_per_checkpoint):\n",
    "    #print(f\"Saving model and optimizer state at iteration {iteration} to {filepath}\")\n",
    "    #torch.save({'iteration': iteration,\n",
    "    #            'state_dict': model.state_dict(),\n",
    "    #            'optimizer': optimizer.state_dict(),\n",
    "    #            'learning_rate': learning_rate}, filepath)\n",
    "    ''' SAVE AT GOOGLE DRIVE '''\n",
    "    try:\n",
    "        itr = (iteration//iters_per_checkpoint)%10\n",
    "        drive_path = drive_path + f'/checkpoint_{itr}'\n",
    "        print(f\"Saving model and optimizer state at iteration {iteration} ({itr}) to {drive_path}\")\n",
    "        torch.save({'iteration': iteration,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'learning_rate': learning_rate}, drive_path)\n",
    "    except Exception:\n",
    "        print('EXCEPT WHEN SAVING CHECKPOINT AT GOOGLE DRIVE')\n",
    "    ''' SAVE AT GOOGLE DRIVE '''\n",
    "\n",
    "\n",
    "''' PLOTTING '''\n",
    "def plot_alignment(alignment, info=None):\n",
    "    alignment_graph_height = 600\n",
    "    alignment_graph_width = 1000\n",
    "    %matplotlib inline\n",
    "    fig, ax = plt.subplots(figsize=(int(alignment_graph_width/100), int(alignment_graph_height/100)))\n",
    "    im = ax.imshow(alignment, cmap='inferno', aspect='auto', origin='lower',\n",
    "                   interpolation='none')\n",
    "    ax.autoscale(enable=True, axis=\"y\", tight=True)\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    xlabel = 'Decoder timestep'\n",
    "    if info is not None:\n",
    "        xlabel += '\\n\\n' + info\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Encoder timestep')\n",
    "    plt.tight_layout()\n",
    "    fig.canvas.draw()\n",
    "    plt.show()\n",
    "''' PLOTTING '''\n",
    "\n",
    "\n",
    "def validate(model, criterion, valset, iteration, batch_size, n_gpus,\n",
    "             collate_fn, logger, distributed_run, rank):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loader = DataLoader(valset, sampler=None, num_workers=1,\n",
    "                                shuffle=False, batch_size=batch_size,\n",
    "                                pin_memory=False, collate_fn=collate_fn)\n",
    "        val_loss = 0.0\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            x, y = model.parse_batch(batch)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            reduced_val_loss = loss.item()\n",
    "            val_loss += reduced_val_loss\n",
    "        val_loss = val_loss / (i + 1)\n",
    "    model.train()\n",
    "    if rank == 0:\n",
    "        print(\"Validation loss {}: {:9f}  \".format(iteration, val_loss))\n",
    "        logger.log_validation(val_loss, model, y, y_pred, iteration)\n",
    "        ''' PLOTTING '''\n",
    "        %matplotlib inline\n",
    "        _, mel_outputs, gate_outputs, alignments = y_pred\n",
    "        idx = random.randint(0, alignments.size(0) - 1)\n",
    "        plot_alignment(alignments[idx].data.cpu().numpy().T)\n",
    "        ''' PLOTTING '''\n",
    "\n",
    "\n",
    "def train(output_directory, log_directory, checkpoint_path, warm_start, \n",
    "          hparams, drive_path,\n",
    "          n_gpus=1, rank=0, group_name='group_name'):\n",
    "    print('START TRAIN')\n",
    "    torch.manual_seed(hparams.seed)\n",
    "    torch.cuda.manual_seed(hparams.seed)\n",
    "    model = load_model(hparams)\n",
    "    learning_rate = hparams.learning_rate\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                                 weight_decay=hparams.weight_decay)\n",
    "    criterion = Tacotron2Loss()\n",
    "    logger = prepare_directories_and_logger(\n",
    "        output_directory, log_directory, rank)\n",
    "    train_loader, valset, collate_fn = prepare_dataloaders(hparams)\n",
    "    iteration = 0\n",
    "    epoch_offset = 0\n",
    "    if checkpoint_path is not None:\n",
    "        if warm_start:\n",
    "            model = warm_start_model(\n",
    "                checkpoint_path, model, hparams.ignore_layers)\n",
    "        else:\n",
    "            model, optimizer, _learning_rate, iteration = load_checkpoint(\n",
    "                checkpoint_path, model, optimizer)\n",
    "            if hparams.use_saved_learning_rate:\n",
    "                learning_rate = _learning_rate\n",
    "            iteration += 1\n",
    "            epoch_offset = max(0, int(iteration / len(train_loader)))\n",
    "    model.train()\n",
    "    is_overflow = False\n",
    "    # ================ MAIN TRAINNIG LOOP! ===================\n",
    "    for epoch in range(epoch_offset, hparams.epochs):\n",
    "        print(\"Epoch: {}\".format(epoch))\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            start = time.perf_counter()\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = learning_rate\n",
    "            x, y = model.parse_batch(batch)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            reduced_loss = loss.item()\n",
    "            loss.backward()\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), hparams.grad_clip_thresh)\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "            if not is_overflow and rank == 0:\n",
    "                duration = time.perf_counter() - start\n",
    "                print(\"Train loss {} {:.6f} Grad Norm {:.6f} {:.2f}s/it\".format(\n",
    "                    iteration, reduced_loss, grad_norm, duration))\n",
    "                logger.log_training(\n",
    "                    reduced_loss, grad_norm, learning_rate, duration, iteration)\n",
    "            if not is_overflow and (iteration % hparams.iters_per_checkpoint == 0):\n",
    "                validate(model, criterion, valset, iteration,\n",
    "                         hparams.batch_size, n_gpus, collate_fn, logger,\n",
    "                         hparams.distributed_run, rank)\n",
    "                if rank == 0:\n",
    "                    checkpoint_path = os.path.join(\n",
    "                        output_directory, \"checkpoint_{}\".format(iteration))\n",
    "                    save_checkpoint(model, optimizer, learning_rate, iteration,\n",
    "                                    checkpoint_path, drive_path, hparams.iters_per_checkpoint)\n",
    "            if i % 100 == 0:\n",
    "                print(i, 'of', len(train_loader))\n",
    "            iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "KJAwvPNKVSrS"
   },
   "outputs": [],
   "source": [
    "hparams = create_hparams()\n",
    "torch.backends.cudnn.enabled = hparams.cudnn_enabled\n",
    "torch.backends.cudnn.benchmark = hparams.cudnn_benchmark\n",
    "print(\"FP16 Run:\", hparams.fp16_run)\n",
    "print(\"Dynamic Loss Scaling:\", hparams.dynamic_loss_scaling)\n",
    "print(\"Distributed Run:\", hparams.distributed_run)\n",
    "print(\"cuDNN Enabled:\", hparams.cudnn_enabled)\n",
    "print(\"cuDNN Benchmark:\", hparams.cudnn_benchmark)\n",
    "\n",
    "# Локальные директории для сохранения логов и чекпоинтов\n",
    "outdir = 'outdir'\n",
    "logdir = 'logdir'\n",
    "\n",
    "# Если обучатся будет уже предобученная модель, то необходимо загрузить данный checkpoint.pt на Гугл Диск \n",
    "# ('/content/drive/MyDrive/data/checkpoint_8800' - в данном случае в папку data, где расположен zip архив мел-спектограмм)\n",
    "checkpoint_path = '/content/drive/MyDrive/data/checkpoint_8800' # None\n",
    "warm_start = True # False\n",
    "\n",
    "# Ошибки переполнения gpu памяти при обучении на Tesla-T4 не возникало\n",
    "hparams.batch_size = 64\n",
    "\n",
    "# Как часто будет сохраняться модель (каждое сохранение ~300мб)\n",
    "hparams.iters_per_checkpoint = 20\n",
    "\n",
    "# HIFITTS датасет состоит из 10 спикеров\n",
    "hparams.n_speakers = 10\n",
    "\n",
    "# Данный параметр лучше подобрать самостоятельно\n",
    "hparams.speakers_embedding_dim = 2\n",
    "\n",
    "# Необходимо создать директорию /logs в Гугл Диске\n",
    "drive_path = '/content/drive/MyDrive/logs'\n",
    "# Необходимо загрузить данные файлы (train.txt и test.txt) в директорию /tacotron2/filelists\n",
    "hparams.training_files = 'filelists/train.txt'\n",
    "hparams.validation_files = 'filelists/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88pIRtyXp1gA"
   },
   "outputs": [],
   "source": [
    "# Запуск обучения\n",
    "train(outdir, logdir, checkpoint_path, warm_start, hparams, drive_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWo4fo7BfdgG"
   },
   "source": [
    "Для избежания блокировки Collab в режиме бездействия (на бесплатной версии), необходимо вставить и запустить нижеуказанный код в консоли браузера (F12 - открыть режим разработчика во многих браузерах):\n",
    "\n",
    "```\n",
    "function ClickConnect(){\n",
    "    console.log(\"Clicked on connect button\"); \n",
    "    document.querySelector(\"colab-connect-button\").click()\n",
    "}\n",
    "setInterval(ClickConnect,60000)\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
